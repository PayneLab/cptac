Note: In the process of updating a dataset, we want to be very careful that we don't break things for current users of the old dataset while we're working on the update, and we want to make sure that after they have the updated package and data, they can still access the old data. I'll reiterate the following points in context in the instructions below, but to achieve these goals, make sure to do the following:



*   **Don't delete the old data files from Box**--leave them exactly as they are.
*   **Don't upload the new version of the index and index hash until the new package version is released on PyPI.** Otherwise, the package will be broken for all current users, because it will try to load the new data files, but it won't have the code for handling them, and will almost always run into errors.
*   **If/when you make ANY changes to the dataset's class's __init__ function, don't delete whatever the old code was.** We need to preserve it, so the package can still load the old data versions. Instead, create an `if` statement that checks the self._version attribute, and if it's the old data version, runs the old code; else, if it's the new data version, run the new code.

Steps for updating an existing dataset:



1. During the next several steps, you will be writing the code to load the new data version files. You want to be able to run this on your machine, so you can test the code as you work on it. However, you can't automatically do that, because when you create the dataset, the __init__ function will automatically update the dataset index, which will download the old version of the index, as stored on Box, which you don't want. However, you can't update the index on Box yet, because that would break the package for all current users trying to access the dataset.

    So, while you're working on the dataset __init__ function, you need to go to the file cptac/dataset.py and comment out the try/except block at the beginning of the function that calls update_index. Also comment out the line of code that validates the file version, since we don't have the indices necessary to do that; below it, add this line and comment: `self._version = version # TEMP FOR DEV; REPLACE W/ ABOVE WHEN DONE`.

2. Go through the raw data files from the new data release, and select which ones to download.
    1. For data files that were already included in the previous data version, you will often just use the corresponding file from the new data freeze. However, sometimes the new data freeze will have additional versions of the same data file, perhaps allowing you to get a version of the table generated by a different lab, using a different data transformation, or that groups the data in a different way. Consult the data freeze's README file to know which file is recommended for use in analysis--that's the one we want. Confirm with Dr. Payne.
    2. There also may be new data types that weren't included in the previous data version at all. For incorporating these new files, again consult the data freeze's README to know which version of the file to use, and confirm with Dr. Payne.
3. After downloading the files, gzip them (unless they're Excel files--leave those uncompressed).
4. Within the data directory for that dataset in the copy of the git repository on your local machine, create a directory for the new version. Name it with the format `[dataset]_v[version]`. For example, if you were adding version 2.1 of the endometrial dataset, you'd create the directory `endometrial_v2.1/` within the directory `cptac/cptac/data_endometrial/`. Copy the selected data files for the new version into this directory.
5. In the following steps, if you need to replace any existing code for loading and parsing the data, don't delete whatever the old code was. We need to preserve it, so the package can still load the old data versions. Instead, create an `if` statement that checks the self._version attribute, and if it's the old data version, runs the old code; else, if it's the new data version, run the new code.
6. Add the version number for the new data version to the valid_versions list at the beginning of the dataset's __init__ function.
7. Edit the `data_files` dictionary created at the beginning of the dataset's class's __init__ function, so that it contains a key that is the new version number, with a value that is a list of all the files in the new data version.
8. Write new `if`/`elif` statements in the dataframe loading portion of the __init__ function to load and parse any data files that weren't included in the previous data version. In those statements, add an additional condition that self._version match the new data version, so that the block won't be executed if they're loading the old version.
    1. As will be mentioned again in step 9, make sure that these new tables are formatted properly. For formatting details, look at the dataframes from previous versions for examples, and consult the "add_new_dataset" document.
        1. You may encounter tables that need a multi-level column index. Consult the "Mulit-level column indices" section of the "why_we_did_what_we_done" document for more details, if you're unfamiliar with the Pandas MultiIndex.
    2. None of the column headers should be duplicated.
9. As necessary, edit the `if`/`elif` statements for data files that were already included in the previous data release. 
    1. You can usually use most of the same loading code to load and parse these files, if their format didn't change drastically. To check for differences, I recommend manually reading in the old and new versions of the file in a Python interpreter, using the pandas read_csv function, and comparing the two versions of the files. You can also manually type in the old loading and parsing code, but use it on the new file, and see if it works (or do this with the Python debugger).
    2. If you're using the same file between two versions, but the file name has changed, you'll have to edit the if statement so that it will accept both file names. So, instead of something like `if file_name == old_file_name:` you can say `if file_name in [old_file_name, new_file_name]:`
    3. As mentioned above, if you do need to make changes, **don't delete the old code**, because we still want to be able to load the old files. Instead, add an `if` statement that checks the data version (stored in the self._version attribute of the dataset class), and executes the old code if we're loading the old version, or executes the new code if we're loading the new version. For examples of this, look at cptac/gbm.py.
    4. There may have been conditional statements that were based on the version of the data--they would look like

    ```
    if self._version == "2.0":
        # Process older file version...
    elif self._version == "2.5":
        # Process new file version...
    ```

    If you were updating to data version 3.0, for example, but the particular file was the same as in version 2.5, you'd need to edit the conditional to look like the following, so that it would still process the file the same way as version 2.5:

    ```
    if self._version == "2.0":
        # Process older file version...
    elif self._version in ["2.5", "3.0"]:
        # Process new file versionâ€¦
    ```


        To find all places where you'd need to make the edit, you could just do a search for the string `_version`.

10. Make sure that new data files are processed properly:
    1. If any columns were being dropped from the old dataframe, make sure we should still drop those columns from the new one.
    2. Compare the patient ids and sample ids included in the new vs. old data, and consider any ramifications of a new sample being included, or a previously included sample no longer being included.
    3. Make sure to check all tables for duplicate column headers or rows, additional indexing columns, etc. For full formatting details, look at the dataframes from previous versions for examples, and consult the "Dataframe formatting requirements" section in the "add_new_dataset" document. 
11. Check the possibilities for values in the "Mutation" column of the somatic_mutation dataframe. The standard possibilities are ['Frame_Shift_Del', 'Frame_Shift_Ins', 'Nonsense_Mutation', 'Nonstop_Mutation', 'Splice_Site', 'In_Frame_Del', 'In_Frame_Ins', 'Missense_Mutation']. 'Silent' is optional. If the new mutation file uses different categories, you will need to add an elif statement in the DataSet._filter_multiple_mutations function to handle that. This also affects the DataSet.get_genotype_all_vars function, and the utils.get_frequently_mutated function.
12. Make a new index file and index hash. However, instead of creating a new index file, append the new indexing information to the end of the existing index file, to create something like this:

![endo_multi_version_index](imgs/endo_multi_version_index.png)
 (Note that even if a file stays exactly the same between two data versions, we still have a different copy of it in each version's data directory on Box, and thus have a unique shared URL for that file in that data version.)
13. **IMPORTANT:** Uncomment the lines of code in the cptac/dataset.py file that you commented out in step one, and delete the temporary line of code.
14. Release the new version of the package on GitHub and PyPI, following all of the instructions in the "release_new_package_version" document. Make sure to update the `version.txt` file on Box so users will know they need to update the package. Then, immediately after releasing the new package on PyPI, upload the updated dataset's index and index_hash files on Box. 
    1. Make sure to update the files on Box quickly, because until the index and index hash files are updated, the package will be broken for anyone who tries to use the new package version. 
    2. While updating the files on Box, make sure to use the "Upload New Version" button for the existing versions of the files, instead of separately uploading the new versions of files, so that the shared URLs for the files don't change. If the URLs changed, it would make it so no one could download the files with the URLs embedded in the package.
